# 구글 드라이브 설계

- 설계에 들어가기 앞서 일단 구글 드라이브가 어떤 서비스인지 알아보자.  
  구글 드라이브는 파일 저장 및 동기화 서비스로 문서, 사진, 비디오, 기타 파일을 클라우드에 보관할 수 있도록 한다.  
  이 파일은 컴퓨터, 스마트콘, 태블릿 등 어떠한 단말에서도 이용 가능해야 한다. 아울러 보관된 파일은 친구, 가족, 동료 등  
  다른 사용자와 손쉽게 공유할 수 있어야 한다.

## 문제 이해 및 설계 범위 확정

> 구글 드라이브를 설계하는 것은 매우 큰 프로젝트이니, 질문을 통해 설계 범위를 좁히도록 하자.

- 아래는 가장 중요하게 지원해야할 기능 및 요구사항이다.

  - 가장 중요하게 지원해야 할 기능: 파일 업로드/다운로드, 파일 동기화, 알림
  - 모바일 앱, 웹 앱 모두 지원
  - 파일 암호화가 필요하다.
  - 파일 크기는 10GB의 제한이 있다.
  - DAU는 1000만이다.

- 이번 장에서는 아래 기능들의 설계에 집중할 것이다.

  - 파일 추가: ex) drag-and-drop
  - 파일 다운로드
  - 여러 단말에 파일 동기화. 한 단말에서 파일을 추가하면 다른 단말에도 자동으로 동기화되어야 한다.
  - 파일 갱신 이력 조회(revision history)
  - 파일 공유
  - 파일이 편집되거나 삭제되거나 새롭게 공유되었을 때 notification

- 이번 장에서 다루지 않을 기능들은 아래와 같다.

  - Google Docs 편집 및 협업 기능

- 기능적 요구사항 이외에 아래의 비 기능적 요구사항을 이해하는 것도 중요하다.

  - 안정성: 저장소 시스템에서 안정성은 아주 중요하다. 데이터 손실은 발생하면 안된다.
  - 빠른 동기화 속도: 파일 동기화에 시간이 너무 많이 걸리면 UX를 매우 손상시킬 것이다.
  - 네트워크 대역폭: 이 제품은 네트워크 대역폭을 불필요하게 많이 소모하면 안된다. 이또한 UX를 해칠 수 있다.
  - 규모 확장성: 이 시스템은 아주 많은 양의 트래픽도 처리할 수 있어야 한다.
  - 높은 가용성: 일부 서버에 장애가 발생하거나, 느려지거나, 네트워크 일부가 끊겨도 시스템은 계속 사용 가능해야 한다.

### 개략적 추정치

- 가입 사용자는 5000만명이고, DAU는 1000만명이다.
- 모든 사용자에게 10GB의 무료 저장 공간을 할당할 수 있어야 한다.
- 매일 각 사용자가 평균 2개의 파일을 업로드한다고 가정한다. 각 파일의 평균 크기는 500KB 이다.
- 읽기, 쓰기 연산의 비율은 1:1이다.
- 필요한 저장공간의 총량은 `5000만 사용자 * 10GB = 500Petabyte` 이다.
- 업로드 API의 QPS는 `1천만 사용자 * 2회 업로드 / 24시간/3600초 = 약 240` 이다.
- 최대 QPS는 `QPS * 2 = 480` 이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 이번에는 모든 것을 담은 한 대의 서버에서 출발해 점진적으로 천만 사용자 지원이 가능한 시스템으로 발전시켜 나가보자.

- 우선 아래와 같은 구성의 서버 한 대로 시작해보자.

  - 파일을 올리고 다운로드 하는 과정을 처리하는 웹 서버
  - 사용자 데이터, 로그인 정보, 파일 정보 등의 메타데이터를 보관할 데이터베이스
  - 파일을 저장할 저장소 시스템. 파일 저장을 위해 1TB의 공간을 사용할 것이다.

- 몇 시간 정도 들여서 Apache Web Server를 설치하고, MySQL Database를 깔고, 업로드되는 파일을 저장할 `drive/`라는 디렉토리를 준비한다.  
  `drive/` 디렉토리 하위에는 namespace라 불리는 하위 디렉토리들을 둔다. 각 namespace 내에는 특정 사용자가 올리는 파일들이 보관된다.  
  이 파일들은 원래 파일명과 같은 이름을 갖는다. 각 파일과 폴더는 그 상대 경로를 namespace 이름과 결합하면 unique하게 식별해낼 수 있다.

- 아래 그림은 `drive/` 디렉토리에 실제 파일이 보관된 사례를 나타낸다.

![picture 5](/images/SDI_GD_1.png)

### API

- 이 시스템은 어떤 API를 제공해야 할까? 기본적으로 3개의 API가 필요하다.  
  파일 업로드 API, 파일 다운로드 API, 그리고 파일 갱신 히스토리 제공 API이다.

#### 1. 파일 업로드 API

- 이 시스템은 두 가지 종류의 업로드를 제공한다.

  - 단순 업로드: 파일 크기가 작을 때 사용한다.
  - Resumable upload: 파일 사이즈가 크고 네트워크 문제로 업로드가 중단될 가능성이 높다고 생각되면 사용한다.

- Endpoint: `/files/upload?uploadType=resumable`

- 인자

  - uploadType=resumable
  - data: 업로드할 로컬 파일

- Resumable upload는 아래의 세 단계 절차로 이뤄진다.

  - Resumable upload URL을 받기 위한 최초 요청 전송
  - 데이터를 업로드하고 업로드 상태 모니터링
  - 업로드에 장애가 발생하면 장애 발생 지점부터 업로드를 재시작

#### 2. 파일 다운로드 API

- Endpoint: `/files/download`

- 인자

  - path: 다운로드 할 파일의 경로

    ```json
    {
      "path": "/recipes/soup/best_soup.txt"
    }
    ```

#### 3. 파일 갱신 히스토리 API

- Endpoint: `/files/list_revisions`

- 인자

  - path: 갱신 히스토리를 가져올 파일의 경로
  - limit: 히스토리 길이의 최대치

  ```json
  {
    "path": "/recipes/soup/best_soup.txt",
    "limit": 20
  }
  ```

- 지금까지 본 모든 API는 사용자 인증을 필요로 하고 HTTPS 프로토콜을 사용해야 한다.  
  SSL(Secure Socket Layer)를 지원하는 프로토콜을 이용하는 이유는 클라이언트와 백엔드 서버가 주고받는 데이터를 보호하기 위함이다.

### 한 대 서버의 제약 극복

- 업로드되는 파일이 많아지다 보면, 결국에는 파일 시스템이 가득 차게 된다.

- 파일 시스템의 여유 공간이 정말 조금 남은 상황이 되었다고 가정해보자. 이렇게 되면 사용자는 더 이상 파일을 올릴 수 없게 되므로,  
  긴급히 문제를 해결해야 한다. 가장 먼저 떠오르는 해결책은 데이터를 sharding해 여러 서버에 나누어 저장하는 것이다.  
  아래 그림은 `user_id`를 기준으로 sharding한 예시이다.

![picture 6](/images/SDI_GD_2.png)

- 더 나아가 업계 최고 수준의 규모 확장성, 가용성, 보안, 성능을 제공하는 객체 저장소 서비스인 Amazon S3를 도입하기로 했다 해보자.  
  S3는 replication을 지원하는데, 같은 region 내에서 replication을 할 수도 있고 여러 region에 걸쳐 replication을 할 수도 있다.  
  AWS 서비스 region은 AWS가 데이터 센터를 운영하는 지리적 영역이다. 아래 그림에서 볼 수 있듯이, 데이터 replication을 할 때는 같은  
  region 내에서만 할 수도 있고, 여러 region에 걸쳐서도 할 수 있다. 여러 region에 걸쳐 replication을 하면 데이터의 손실을 막고  
  가용성을 최대한 보장할 수 있으므로 이렇게 하기로 하자.

![picture 7](/images/SDI_GD_3.png)

- 파일을 S3에 넣고 나니, 저장 공간 부족으로 인한 데이터 손실 등의 장애는 더 이상 발생하지 않을 것이다.  
  조금 더 개선할 부분들을 보자.

  - Load balancer: 네트워크 트래픽을 분산하기 위해 load balancer를 사용한다. Load balancer는 트래픽을 고르게 분산할 수 있을  
    뿐만 아니라, 특정 웹 서버에 장애가 발생하면 자동으로 해당 서버를 우회해준다.

  - Web server: Load balancer를 추가하고 나면 더 많은 웹 서버를 손쉽게 추가할 수 있다. 따라서 트래픽이 폭증해도 쉽게 대응이 가능하다.

  - Metadata Database: 데이터베이스를 파일 저장 서버에서 분리해 SPOF를 회피한다. 아울러 replication과 sharding 정책을 사용해  
    가용성과 규모 확장성 요구사항에 대응한다.

  - File storage: S3를 파일 저장소로 사용하고 가용성과 데이터 무손실을 보장하기 위해 2개 이상의 region에 데이터를 replicate 한다.

- 이 모든 부분을 개선하고 나면 web server, metadata database, file storage가 한 대의 서버에서 여러 서버로 잘 분리되었을 것이다.  
  아래 그림은 이에 맞게 수정된 설계안이다.

![picture 8](/images/SDI_GD_4.png)

### 동기화 충돌

- 구글 드라이브와 같은 대형 저장소 시스템의 경우, 때때로 동기화 충돌이 발생할 수 있다.  
  이를테면 두 명 이상의 사용자가 같은 파일이나 폴더를 동시에 업데이트하려고 하는 경우이다. 이런 충돌은 어떻게 해소할 수 있을까?  
  여기서는 다음의 전략을 선택할 것인데 먼저 처리되는 변경은 성공한 것으로 보고, 나중에 처리되는 변경은 충돌이 발생한 것으로 표시하는 것이다.

![picture 9](/images/SDI_GD_5.png)

- 위 그림에서 사용자1과 사용자2는 같은 파일을 동시에 갱신하려 한다. 하지만 이 시스템은 사용자1의 파일을 먼저 처리했다.  
  따라서 사용자1의 파일 갱신 시도는 정상적으로 처리되지만, 사용자2에 대해서는 동기화 충돌 오류가 발생할 것이다.  
  그럼 이 오류는 어떻게 해결해야 할까? 오류가 발생한 시점에 이 시스템에는 같은 파일의 두 가지 버전이 존재하게 된다.  
  즉, 사용자2가 갖고 있는 local copy와 서버에 있는 최신 버전이다. 이 상태에서 사용자는 두 파일을 하나로 합칠지, 아니면 둘 중  
  하나를 다른 파일로 대체할지를 결정해야 한다.

### 개략적 설계안

- 아래 그림은 이번에 구축할 구글 드라이브의 개략적 설계안이다.

![picture 10](/images/SDI_GD_6.png)

- 각 컴포넌트에 대해 조금 더 자세히 알아보자.

  - 사용자 단말: 사용자가 이용하는 웹 브라우저나 모바일 앱 등의 클라이언트
  - 블록 저장소 서버(Block server): 파일 블록을 클라우드 저장소에 업로드하는 서버다. 블록 저장소는 블록 수준 저장소(block-level storage)라고도  
    하며, 클라우드 환경에서 데이터 파일을 저장하는 기술이다. 이 저장소는 파일을 여러 개의 블록으로 나눠 저장하며 각 블록에는 고유한 hash값이 할당된다.  
    이 hash값은 Metadata database에 저장된다. 각 블록은 독립적인 객체로 취급되며, 클라우드 저장소 시스템(S3)에 보관된다.  
    파일을 재구성하려면 블록들을 원래 순서대로 합쳐야 한다.

  - 클라우드 저장소: 파일은 블록 단위로 나뉘어져 클라우드 저장소에 보관된다.
  - 아카이빙 저장소(Cold storage): 오랫동안 사용되지 않은 inactive 데이터를 저장하기 위한 컴퓨터 시스템이다.
  - Load balancer: 요청을 모든 API server에 고르게 분산하는 역할을 한다.
  - API Server: 파일 업로드 외의 거의 모든 것을 담당하는 서버다. 사용자 인증, 프로필 관리, 파일 메타데이터 갱신 등에 사용된다.
  - Metadata Database: 사용자, 파일, 블록, 버전 등의 메타데이터 정보를 관리한다. 실제 파일은 클라우드에 보관하며, 이 데이터베이스에는  
    오직 메타데이터만 둔다는 것을 명심하자.
  - Metadata Cache: 성능을 높이기 위해 자주 쓰이는 메타데이터는 cache한다.
  - 알림 서비스: 특정 이벤트가 발생했음을 클라이언트에게 알리는 데 쓰이는 publish/subscribe 프로토콜 기반 시스템이다.  
    이 설계안에서는 클라이언트에게 파일이 추가되거나, 편집되거나, 삭제되었음을 알려 파일의 최신 상태를 확인하도록 하는 데 쓰인다.
  - 오프라인 사용자 백업 큐(offline backup queue): 클라이언트가 접속 중이 아니라서 파일의 최신 상태를 확인할 수 없을 때는 해당 정보를 이 queue에  
    두어 나중에 클라이언트가 접속했을 때 동기화할 수 있도록 한다.

---

## 상세 설계

### 블록 저장소 서버

- 정기적으로 갱신되는 큰 파일들은 업데이트가 일어날 때마다 전체 파일을 서버로 보내면, 네트워크 대역폭을 많이 잡아먹게 된다.  
  이를 최적화하는 방법으로는 두 가지 정도를 생각해볼 수 있다.

  - Delta sync: 파일이 수정되면 전체 파일 대신 수정이 일어난 블록만 동기화한다.
  - Compression: 블록 단위로 압축해 두면 데이터 크기를 많이 줄일 수 있다. 이때 압축 알고리즘은 파일 유형에 따라 정한다.  
    예를 들어 텍스트 파일이면 gzip이나 bzip2를 쓰고, 이미지나 비디오인 경우에는 다른 압축 알고리즘을 사용할 수 있다.

- 이 시스템에서 블록 저장소 서버는 파일 업로드에 관계된 힘든 일을 처리하는 컴포넌트이다. 클라이언트가 보낸 파일을 블록 단위로 나눠야 하고,  
  각 블록에 압축 알고리즘을 적용해야 하고, 암호화까지 해야 한다. 아울러 전체 파일을 저장소 시스템으로 보내는 대신, 수정된 블록만 전송해야 한다.

- 새로운 파일이 추가되었을 때 블록 저장소 서버가 어떻게 동작하는지 아래 그림을 통해 살펴보자.

![picture 11](/images/SDI_GD_7.png)

- 순서는 아래와 같다.

  - 주어진 파일을 작은 블록들로 분할한다.
  - 각 블록을 압축한다.
  - 클라우드 저장소로 보내기 전에 암호화한다.
  - 클라우드 저장소로 보낸다.

- 아래 그림은 Delta sync가 어떻게 동작하는지 보여준다. 검정색으로 표시된 블록 2, 5는 수정된 블록이다. 갱신된 부분만 동기화해야 하므로  
  이 두 블록만 클라우드 저장소에 업로드하면 된다.

![picture 12](/images/SDI_GD_8.png)

- 블록 저장소 서버에 delta sync 전략과 압축 알고리즘을 도입했으므로, 네트워크 대역폭 사용량을 절감할 수 있다.

### 높은 일관성 요구사항

- 이 시스템은 Strong Consistency(강한 일관성) 모델을 기본으로 지원해야 한다. 같은 파일이 단말이나 사용자에 따라 다르게 보이는 것을  
  허용하지 않는다는 뜻이다. Metadata cache와 Metadata database 계층에도 같은 원칙이 적용되어야 한다.

- 메모리 cache는 보통 Eventual Consistency model을 지원하낟. 따라서 Strong consistency를 달성하려면 아래 사항을 보장해야 한다.

  - Cache에 보관될 사본과 데이터베이스에 있는 원본(master)이 일치한다.
  - 데이터베이스에 보관된 원본에 변경이 발생하면 cache에 있는 사본을 무효화한다.

- RDBMS는 ACID를 보장하므로 strong consistency를 보장하기 쉽다. 하지만 NoSQL 데이터베이스는 이를 기본으로 지원하지 않으므로,  
  동기화 로직 안에 프로그래밍해 넣어야 한다. 이 설계안에서는 ACID를 기본 지원하는 RDBMS를 선택해 strong consistency 요구사항에 대응할 것이다.

### Metadata Database

![picture 13](/images/SDI_GD_9.png)

- 위 그림은 스키마 설계안이다. 중요한 것만 간추린 아주 단순화된 형태의 스키마임에 유의하자.

  - `user`: `user` 테이블에는 이름, 이메일, 프로필 사진 등 사용자에 관계된 기본적 정보들이 보관된다.
  - `device`: `device` 테이블에는 단말 정보가 보관된다. `push_id`는 모바일 push 알림을 보내고 받기 위한 것이다.  
    한 사용자가 여러 개의 단말을 가질 수 있음에 유의하자.
  - `namespace`: `namespace` 테이블에는 사용자의 root directory 정보가 보관된다.
  - `file`: `file` 테이블에는 파일의 최신 정보가 보관된다.
  - `file_version`: 파일의 갱신 이력이 보관되는 테이블이다. 이 테이블에 보관되는 레코드는 정보 readonly이다.  
    이는 갱신 이력이 훼손되는 것을 막기 위한 조치이다.
  - `block`: 파일 블록에 대한 정보를 보관하는 테이블이다. 특정 버전의 파일은 파일 블록을 올바른 순서로 조합하기만 하면 복원해낼 수 있다.

### 업로드 절차

- 사용자가 파일을 업로드하면 어떤 일이 벌어지는지 자세히 살펴보자.

![picture 14](/images/SDI_GD_10.png)

- 위 그림은 두 개의 요청이 병렬적으로 전송된 상황을 보여준다. 첫 번째 요청은 파일 메타데이터를 추가하기 위한 것이고, 두 번째 요청은 파일을  
  클라우드 저장소로 업로드하기 위한 것이다. 이 두 요청은 모두 클라이언트1이 보낸 것이다.

- 파일 메타데이터 추가

  - (1) 클라이언트1이 새로운 파일의 메타데이터를 추가하기 위해 요청을 전송한다.
  - (2) 새 파일의 메타데이터를 데이터베이스에 저장하고 업로드 상태를 _대기중(pending)_ 으로 변경한다.
  - (3) 새로운 파일이 추가되었음을 알림 서비스에 통지한다.
  - (4) 알림 서비스는 관련된 클라이언트(클라이언트2)에게 파일이 업로드되고 있음을 알린다.

- 파일을 클라우드 저장소에 업로드

  - (1) 클라이언트1이 파일을 블록 저장소 서버에 업로드한다.
  - (2) 블록 저장소 서버는 파일을 블록 단위로 쪼개고 각각을 압축하고 암호화한 다음 클라우드 저장소에 전송한다.
  - (3) 업로드가 끝나면 클라우드 스토리지는 완료 callback을 호출한다. 이 완료 callback은 API Server로 전송된다.
  - (4) Metadata Database에 기록된 해당 파일의 상태를 _완료(uploaded)_ 로 변경한다.
  - (5) 알림 서비스에 파일 업로드가 끝났음을 통지한다.
  - (6) 알림 서비스는 관련된 클라이언트(클라이언트2)에게 파일 업로드가 끝났음을 알린다.

- 파일을 수정하는 경우에도 흐름은 비슷하다.

### 다운로드 절차

- 파일 다운로드는 파일이 새로 추가되거나 편집되면 자동으로 시작된다.  
  그렇다면 클라이언트는 다른 클라이언트가 파일을 편집하거나 추가했다는 사실을 어떻게 감지할 수 있을까? 두 가지 방법을 사용한다.

  - A가 접속중이고 다른 클라이언트가 파일을 변경하면, 알림 서비스가 A에게 변경이 발생했으니 새로운 버전을 끌어가야 한다고 알린다.
  - A가 네트워크에 연결된 상태가 아닌 경우에는 데이터를 cache에 보관한다. 해당 클라이언트의 상태가 접속 중으로 바뀌면 그때 해당  
    클라이언트는 새 버전을 가져갈 것이다.

- 어떤 파일이 변경되었음을 감지한 클라이언트는 우선 API Server를 통해 메타데이터를 새로 가져와야 하고, 그 다음에 블록들을 다운받아 파일을  
  재구성해야 한다. 아래 그림은 더 자세한 흐름을 보여준다.

![picture 15](/images/SDI_GD_11.png)

- (1) 알림 서비스가 클라이언트2에게 누군가 파일을 변경했음을 알린다.
- (2) 알림을 확인한 클라이언트2는 새로운 메타데이터를 요청한다.
- (3) API Server는 Metadata Database에게 새로운 메타데이터를 요청한다.
- (4) API Server에게 새로운 메타데이터가 반환된다.
- (5) 클라이언트2에게 새로운 메타데이터가 반환된다.
- (6) 클라이언트2는 새로운 메타데이터를 받는 즉시 블록 다운로드 요청을 전송한다.
- (7) 블록 저장소 서버는 클라우드 저장소에서 블록을 다운로드한다.
- (8) 클라우드 저장소는 블록 서버에게 요청된 블록을 반환한다.
- (9) 블록 저장소 서버는 클라이언트2에게 요청된 블록을 반환한다. 클라이언트2는 전송된 블록을 사용해 파일을 재구성한다.

### 알림 서비스

- 파일의 일관성을 유지하기 위해, 클라이언트는 로컬에서 파일이 수정되었음을 감지하는 순간 다른 클라이언트에게 그 사실을 알려 충돌 가능성을  
  줄여야 한다. 알림 서비스는 그 목적으로 사용된다. 단순하게 보면, 알림 서비스는 이벤트 데이터를 클라이언트들에게 보내는 서비스이다.  
  따라서 아래 두 가지 정도의 선택지가 있다.

  - Long polling: Dropbox가 이 방식을 채택하고 있다.
  - WebSocket: 클라이언트와 서버 사이에 지속적인 통신 채널을 제공함으로써 양방향 통신이 가능하다.

- 둘 다 좋은 방안이지만, 이 설계안의 경우에는 long polling을 사용할 것이다. 그 이유는 아래와 같다.

  - 채팅 서비스와는 달리, 본 시스템의 경우에는 알림 서비스와 양방향 통신이 필요하지 않다. 서버는 파일이 변경된 사실을 클라이언트에게  
    알려줘야 하지만, 반대 방향의 통신은 요구되지 않는다.
  - WebSocket은 실시간 양방향 통신이 요구되는 채팅 같은 상황에 적합하다. 구글 드라이브의 경우 알림을 보낼 일은 그렇게 자주 발생하지 않으며,  
    알림을 보내야 하는 경우에도 단시간에 많은 양의 데이터를 보낼 일은 없다.

- Long polling 방안을 쓰게 되면 각 클라이언트는 알림 서버와 long polling을 위한 연결을 유지하다가, 특정 파일에 대한 변경을 감지하면  
  해당 연결을 끊는다. 이때 클라이언트는 반드시 메타데이터 서버와 연결해 파일의 최신 내역을 다운로드해야 한다. 해당 다운로드 작업이 끝났거나  
  connection timeout 시간에 도달한 경우에는 즉시 새로운 요청을 보내 long polling connection을 복원하고 유지해야 한다.

### 저장소 공간 절약

- 파일 갱신 이력을 보존하고 안정성을 보장하기 위해서는 파일의 여러 버전을 여러 데이터 센터에 보관해야 할 필요가 있다.  
  이런 상황에서 모든 버전을 자주 백업하게 되면 저장용량이 너무 빨리 소진될 가능성이 있다. 이런 문제를 피하고 비용을 절감하기 위해서는  
  보통 아래의 세 가지 방법을 사용한다.

  - de-dupe(중복 제거): 중복된 파일 블록들을 계정 차원에서 제거하는 방법이다. 두 블록이 같은 블록인지는 hash값을 비교해 판단한다.
  - 지능적 백업 전략을 도입한다. 아래와 같은 전략을 생각해볼 수 있다.
    - 한도 설정: 보관해야 하는 파일 버전 개수에 상한을 둔다. 상한에 도달하면 가장 오래된 버전은 버린다.
    - 중요한 버전만 보관: 어떤 파일은 아주 자주 바뀐다. 예를 들어 편집 중인 문서가 업데이트될 때마다 새로운 버전으로 관리한다면  
      짧은 시간 동안 1000개가 넘는 버전이 만들어질 수 있다. 불필요한 버전과 사본이 만들어지는 것을 피하려면 그 가운데 중요한 것만 골라내야 한다.
  - 자주 쓰이지 않는 데이터는 아카이빙 저장소(cold storage)로 옮긴다. 몇달 혹은 수년간 이용되지 않은 데이터가 이에 해당한다.  
    Amazon S3 Glacier와 같은 아카이빙 저장소 이용료는 S3보다 훨씬 저렴하다.

### 장애 처리

- 장애는 대규모 시스템에서는 피할 수 없는 것으로, 설계 시 그 점을 반드시 고려해야 한다.  
  이 시스템에서 주의깊게 봐야 할만한 부류의 장애로는 아래와 같은 것들이 있다.

  - Load balancer 장애: Load balancer에 장애가 발생할 경우 secondary load balancer가 활성화되어 트래픽을 이어받아야 한다.  
    Load balancer끼리는 보통 heartbeat 신호를 주기적으로 보내 상태를 모니터링한다. 일정 시간 동안 heartbeat 신호에 응답하지 않은  
    load balancer는 장애가 발생한 것으로 간주한다.

  - 블록 저장소 서버 장애: 블록 저장소 서버에 장애가 발생했다면 다른 서버가 미완료 상태 또는 대기 상태인 작업을 이어받아야 한다.
  - 클라우드 저장소 장애: S3 bucket은 여러 region에 replicate할 수 있으므로, 한 region에서 장애가 발생했다면 다른 region에서  
    파일을 가져오면 된다.
  - API Server 장애: API Server들은 stateless 서버다. 따라서 load balancer는 API Server에 장애가 발생하면 트래픽을  
    해당 서버로 보내지 않음으로써 장애 서버를 격리할 것이다.
  - Metadata Cache 장애: Metadata cache server도 replicate한다. 따라서 한 node에 장애가 생겨도 다른 node에서 데이터를  
    가져올 수 있다. 장애가 발생한 서버는 새로운 서버로 교체하면 된다.
  - Metadata Database 장애
    - Master 장애: Slave 중 하나를 master로 바꾸고 slave를 새로 하나 추가한다.
    - Slave 장애: 다른 slave가 read 연산을 처리하도록 하고 그동안 장애 서버는 새로운 것으로 교체한다.
  - 알림 서비스 장애: 접속 중인 모든 사용자는 알림 서버와 long polling connection을 하나씩 유지한다. 따라서 알림 서비스는 많은 사용자와의  
    연결을 유지하고 관리해야 한다. 한 대 서버에 장애가 발생하면 많은 사용자가 long polling connection을 다시 만들어야 한다.  
    이때 주의해야할 것은 한 대 서버로 100만개 이상의 접속을 유지하는 것은 가능하지만, 동시에 백만 개의 접속을 _시작_ 하는 것은 불가능하다.  
    따라서 long polling connection을 복구하는 것은 상대적으로 느릴 수 있다.
  - 오프라인 사용자 backup queue 장애: 이 queue 또한 replicate 해둬야 한다. Queue에 장애가 발생하면 구독 중인 클라이언트들은  
    backup queue로 구독 관계를 재설정해야 할 것이다.

---

### 마무리

- 블록 저장소 서버를 두지 않고, 직접 클라이언트가 파일을 클라우드 저장소에 업로드하도록 하면 어떨까?  
  이 방법의 장점으로는 파일 전송을 클라우드 저장소로 직접 하면 되니 업로드 시간이 빨라진다는 것이다.  
  하지만 이 방법에는 몇 가지 단점이 있다.

  - 분할, 압축, 암호화 로직을 클라이언트에 두어야 하므로 플랫폼별로 따로 구현해야 한다. 위에서 본 설계안에서는 이 모두를 블록 저장소 서버라는  
    곳에 모아 뒀으므로 그럴 필요가 없었다.
  - 클라이언트가 해킹 당할 가능성이 있으므로 암호화 로직을 클라이언트 안에 두는 것은 적절하지 않은 선택일 수 있다.

- 또 하나 생각해 볼만한 것은 접속 상태를 관리하는 로직을 별도 서비스로 옮기는 것이다. 그렇게 해서 관련 로직을 알림 서비스로부터 분리해내면,  
  다른 서비스에서도 쉽게 활용할 수 있게 될 것이다.

---
