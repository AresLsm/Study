# Pod

## Pod Lifecycle

- `Pod`에도 생명주기(Lifecycle)이 존재하며, 생성되는 시점부터 소멸되는 시점까지의 과정이 있다.  
  이런 생명주기의 특징은 각 단계마다 수행되는 동작들이 모두 다르다는 점이다.

- `Pod`를 생성한 후, 내용을 보면 status라는 항목이 있다. 이 항목의 내용들은 아래와 같다.

```yml
status:
  phase: Pending
  conditions:
  - type: Initialized
    status: 'True'
    lastProbeTime: null
    lastTransitionTime: '2021-11-08T09:51:43Z'

  ####

  - type: PodScheduled
    status: 'True'
    lastProbeTime: null
    lastTransitionTime: '2021-11-08T09:51:43Z'

  ####
  - type: ContainersReady
    status: 'False'
    lastProbeTime: null
    lastTransitionTime: '2021-11-08T09:51:43Z'
    reason: ContainersNotReady

 ####
 - type: Ready
   status: 'False'
   lastProbeTime: null
   lastTransitionTime: '2021-11-08T09:51:43Z'
   reason: ContainersNotReady
```

- 이 내용을 하나씩 알아보자.  
  `Pod`가 있고, 그 안에 `Status`가 있다. 이 `Status`안에는 `Pod`의 전체 속성을 대표하는 `Phase`라는 속성이 있다.  
  또한 `Pod`가 생성되면서 실행되는 단계들이 있는데, 그 단계들의 상태를 알려주는 `Conditions`라는 속성도 `Status`내에 있다.

- `Pod` 안에는 하나 이상의 `Container`들이 있다. 이 각각의 `Container` 내에는 해당 컨테이너의 상태를 나타내는 `State`가 있다.

### Phase

- `Status`내에 있고 `Pod`의 전체 속성을 대표하는 `Phase`에는 아래의 다섯 가지 상태가 있다.

  - **Pending**
  - **Running**
  - **Succeeded**
  - **Failed**
  - **Unknown**

### Conditions

- `Pod`가 실행되면서 실행되는 단계들의 상태를 알려주는 `Conditions`에는 아래 네 가지 상태가 있다.

  - **Initialized**
  - **PodScheduled**
  - **ContainersReady**
  - **Ready**

- `Conditions`의 세부 내용들을 알려주는 `Reason`이라는 항목도 있다.

  - **ContainersNotReady**
  - **PodCompleted**
  - **ContainersNotInitialized**
  - 기타 등등

- 위에서 본 yml파일에서 `status: 'False'`인 경우, 이 상태가 왜 false인지를 알아야 하기에  
  `Reason`이 추가되어 있으며(reason), 내용을 보면 원인을 파악하기 쉽다.

### ContainerStatuses

- `Pod`내의 `Container`의 상태를 나타내는 `ContainerStatuses`에는 아래의 세 가지 항목이 있다.

  - **Waiting**
  - **Running**
  - **Terminated**

- `ContainerStatuses` 또한 `Condition`과 마찬가지로 세부 내용을 알기 위한 `Reason`이 있다.  
  예시 상황을 보자.

```yml
containerStatuses:
  - name: container
    state:
      waiting:
        reason: ContainerCreating
    lastState: {}
    ready: false
    restartCount: 0
    image: image/init
    imageID: ""
    started: false
```

- `Pod`의 상태 구조를 나타낸 그림이다.

![picture 1](../../images/POD_LIFECYCLE_1.png)

- 이제 `Pod`의 상태를 나타내는 메인 지표인 `Phase`가 어떻게 변하고, 그 변화에 따라 `Pod`내의 `Container`의  
  동작이 어떻게 변하는지 살펴보자.

### Pod Phase - Pending

- `Pending`은 `Pod`의 최초 상태이다. 이 상태일 때 `Container`에서 일어나는 일들을 보자.  
  우선 실제 `Container`가 구동되기 전에 초기화시켜야 하는 내용들이 있을 경우, 그 내용들을 담고 있는  
  `InitContainer`가 있다. 만약 `Volume`이나 보안 세팅을 위해 사전 설정을 해야하는 일이 있을 경우,  
  `Pod` 생성 내용 안에 initContainers라는 항목에 초기화 스크립트를 넣을 수 있으며, 이 스크립트가  
  실제 `Container`보다 먼저 실행되어 스크립트 실행이 성공적으로 끝나거나, 아예 initContainers가  
  없을 경우에는 `Pending.Initialized`가 true로, 뭔가 잘못되었다면 false로 설정된다.

- 다음으로 이 `Pod`가 특정 `Node`에 올라가도록 직접 지정했을 때는 지정된 `Node`에, 아니라면 K8S가  
  알아서 자원 상황에 따라 `Node`를 결정하기도 하는데, 이 작업이 완료되면 `Pending.PodScheduled`가  
  true로 변한다.

- 그 후에는 `Container`에 사용할 이미지를 다운로드하는 동작이 있다.

- 위 세 단계가 실행되는 동안 `ContainerStatus`는 _Waiting_ 이며, reason은 _ContainerCreating_ 이다.

### Pod Phase - Running

- 이제 본격적으로 `Container`들이 기동되면서 `Pod`와 `Container`는 _Running_ 상태가 된다.  
  정상적으로 기동될 수도 있지만, 하나 이상의 컨테이너가 기동 중 문제가 발생해서 재시작될 수도 있다.  
  이때의 `ContainerStatus`는 다시 _Waiting_ 이 되며, reason은 _CrashLoopBackOff_ 이다.

- 정상적으로 기동되든, 컨테이너에 문제가 생겨 재시작되는 중이든 `Pod`는 이러한 `Container`의 상태를  
  *Running*이라 간주하며 `Phase`를 _Running_ 으로 설정한다. 대신 `Container`가 재시작되고 있다면  
  `Conditions`의 ContainerReady와 Ready는 false이다. 이후 모든 컨테이너들이 정상적으로 구동된다면  
  `Conditions`는 true가 된다.  
  따라서 **`Pod`의 `Phase`가 Running이더라도, `Container`들은 항상 정상 구동중이라는 보장이 없다.**  
  **즉, `Pod`뿐만 아니라 `Container`의 상태 또한 지속적으로 모니터링해야 한다.**

- 일반적으로 서비스가 지속적으로 운영되어야 하는 경우, `Pod`의 `Phase` 중 *Running*을 계속 유지해야 할 것이다.

### Pod Phase - Failed, Succeeded

- `Job` 또는 `CronJob`으로 생성된 `Pod`들은 자신의 일을 수행중일 때는 _Running_ 이지만, 일을 모두 마치면  
  `Pod`는 더이상 일을 하지 않는 상태가 되는데, 이때 `Phase`는 _Failed_ 또는 *Succeeded*로 변한다.  
  만약 작업을 하고 있는 `Container` 중 하나라도 문제가 생겨 `Container`의 상태가 *Error*가 되면  
  `Pod`의 `Phase`는 _Failed_ 가 되며, `Container`들이 모두 _Completed_ 로 주어진 일들을 모두 잘 마쳤을 때는  
  *Succeeded*가 된다.

- 이때 `Pod`의 `Condition`도 변하는데, `Phase`가 _Failed_ 이든 _Succeeded_ 이든 모두 ContainersReady는  
  false로, Ready도 false가 된다.

### 기타 상태

- 추가적으로 _Pending_ 중에 모종의 이유로 바로 _Failed_ 가 되는 경우도 있고, _Pending_ 이나 _Running_ 중에  
  통신 장애 등이 발생하면 _Unknown_ 상태로 바뀐다. 이런 장애들이 빠르게 해결되면 다시 _Pending_ 상태가 되지만,  
  계속 지속되면 _Failed_ 가 되기도 한다.

![picture 2](../../images/POD_LIFECYCLE_OVERVIEW.png)

<hr/>

## ReadinessProbe, LivenessProbe

### ReadinessProbe, LivenessProbe

- `Pod`의 `ReadinessProbe`와 `LivenessProbe`를 알아보자. 우선 이들을 어떤 상황에서 쓰는지 보자.  
  `Pod`를 만들면 그 안에 `Container`가 생기고, `Pod`와 `Container`의 상태가 *Running*이 되면서  
  그 안에 있는 애플리케이션도 정상적으로 구동될 것이다. 그리고 `Pod`는 `Service`에 연결되고, 이 `Service`의  
  IP가 외부에 알려지면서 외부에서 접근할 수 있게 된다.

- 이 상황에서 하나의 `Service`에 2개의 `Pod`가 연결되어 있어, 50%씩 트래픽이 나눠진다 가정해보자.  
  (각 `Pod`는 Node1, Node2라는 `Node` 내에 있다.)  
  이 상태에서 Node2가 모종의 이유로 down되면 그 안에 있는 `Pod`또한 장애가 발생해 `Service`로 오는 모든  
  트래픽이 Node1 내의 `Pod`로 전달될 것이다. 이때 Auto Healing에 의해 Node2의 `Pod`는 다른 `Node`(Node3)에  
  재생성되려 할 것인데, 그 과정에서 새로운 `Pod`와 `Container`가 _Running_ 상태가 되면서 `Service`와 연결되는데,  
  **아직 애플리케이션은 구동 준비중(Booting)인 순간이 발생**한다. `Service`와 연결되자마자 애플리케이션의 상태와는  
  관계없이 트래픽이 Node3로도 분배되기 때문에 만약 애플리케이션이 구동중이라면 일부 사용자는 에러가 발생하게 된다.

- 이런 상황에 대비하기 위해 `Pod`를 만들 때 `ReadinessProbe`를 지정하면 이런 문제를 피할 수 있다.  
  `ReadinessProbe`가 애플리케이션이 구동되기 전까지는 `Service`와 연결되지 않게끔 해주기 때문이다.

- 또다른 상황으로 `Node`가 down되는게 아닌, 애플리케이션이 중단되고, `Pod`는 _Running_ 상태라고 해보자.  
  예를 들어 서비스를 Tomcat으로 돌릴 때 Tomcat은 작동하지만, 그 위에 띄워진 애플리케이션에 메모리 초과가 발생한다면  
  애플리케이션에 접근하면 500(INTERNAL SERVER ERROR)가 반환되게 된다. 이 경우에는 Tomcat 프로세스 자체가 죽은게 아니라  
  그 위에 있던 애플리케이션에 문제가 생긴 것이기에 Tomcat의 프로세스를 지켜보는 `Pod`의 입장에서는 계속 _Running_ 상태로  
  있게 되며, 이런 상황이 발생하면 이 `Pod`로 오는 트래픽은 또 문제가 된다.

- 이런 경우에 애플리케이션의 장애를 감지해주는게 바로 `LivenessProbe`인데, 마찬가지로 `Pod`를 만들 때  
  `LivenessProbe`를 지정해주면 해당 애플리케이션에 장애가 생기면 `Pod`를 재실행하게 만들어서  
  잠깐의 트래픽 에러는 발생하지만, 지속적으로 에러 상태에 머물러 있는 것을 방지해준다.

- 이렇게 서비스를 조금 더 안정하게 유지하기 위해서는 `ReadinessProbe`를 지정해 애플리케이션의 구동 순간에  
  발생하는 트래픽 실패를 없애고, `LivenessProbe`를 통해 애플리케이션에 장애 발생 시 지속적인 실패를 예방해야 한다.

- 이제 이 둘의 세부적인 사용 방법을 알아보자.

- `ReadinessProbe`와 `LivenessProbe`는 사용 목적만 다를 뿐, 설정할 수 있는 내용은 같다.  
  공통적으로 들어갈 수 있는 속성들을 보면 대표적으로 httpGet, Exec, tcpSocket으로 해당 애플리케이션의  
  상태를 확인할 수 있다.

  - httpGet: 포트번호, 호스트명, 엔드포인트(path), header, scheme(http, https 등)을 지정해  
    애플리케이션 상태 확인

  - Exec: 특정 명령어(Command)를 수행해 그 결과를 확인

  - tcpSocket: 포트번호, 호스트명을 검사

- 위 세 개 중 하나는 필수적으로 지정해야 하며 아래와 같은 옵션값들이 있다.

  - initialDelaySeconds: 최초 probe 수행 전 딜레이 시간(default: 0초)
  - periodSeconds: probe 수행 간격(default: 10초)
  - timeoutSeconds: 결과를 기다리는 최대 시간(default: 1초)
  - successThreshold: 정상 처리로 간주하기 위한 성공 횟수(default: 1회)
  - failureThreshold: 실패 처리로 간주하기 위한 실패 횟수(default: 3회)

### `ReadinessProbe`

- 하나의 `Service`에 `Pod`가 연결되어 있는 상태에서 `Pod`를 하나 더 만들 것인데, `Container`의 hostPath로  
  `Node`의 `Volume`이 연결되어 있다 해보자.

![picture 3](../../images/READINESS_PROBE_1.png)

- 새로 만들 `Container`에 `ReadinessProbe`를 설정해보자.  
  위에서 본 설정값 중 Exec를 사용해보자. 명령어(Command)는 `cat /readiness/ready.txt`를 수행할 것이며  
  옵션으로는 initialDelaySeconds는 5, periodSeconds는 10, 그리고 successThreshold는 3을 지정해보자.

- 이렇게 설정하면 `Pod`를 만들 때 `Node`가 스케줄되고 `Container`가 사용할 이미지가 다운로드되면서  
  `Pod`와 `Container`의 상태는 _Running_ 이 되지만, `ReadinessProbe`에 지정한 probe가 성공하기 전까지  
  `Condition`의 ContainerReady와 Ready는 false로 유지된다. 만약 이 상태가 계속 false로 유지되면  
  `Endpoint`에서는 이 `Pod`의 IP를 NotReadyAddr로 간주하고 `Service`에 연결하지 않는다.  
  그 다음에 K8S가 `ReadinessProbe`에 정의된 동작대로 애플리케이션의 기동 상태를 체크하게 되는데, 지정한 것처럼  
  `Container`가 _Running_ 상태가 되면 최초 5초동안 지연하다가 10초에 한 번씩 `cat /readiness/ready.txt`를 수행한다.  
  계속해서 이 명령어가 잘못된 결과를 반환하면 이 `Pod`의 Ready는 false로 유지되고, 3번 성공을 확인하면  
  Ready가 true가 되고 `Endpoint`도 이 `Pod`의 IP를 정상적으로 Address로 간주하면서 `Service`와 연결된다.

- yml 파일은 아래와 같다.

```yml
# yml for Pod
apiVersion: v1
kind: Pod
metadata:
  #..
spec:
  containers:
    - name: app1
      image: image/app
      ports:
        - containerPort: 8080
      readinessProbe:
        exec:
          command: ["cat", "/readiness/ready.txt"]
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 3
      volumeMounts:
        - name: host-path
          mountPath: /readiness
  volumes:
    - name: host-path
      hostPath:
        path: /tmp/readiness
        type: DirectoryOrCreate
```

### `LivenessProbe`

- 이번에는 `LivenessProbe`를 살펴보자. 하나의 `Service`에 두 개의 `Pod`가 _Running_ 상태로 있다.  
  여기에서 수행되는 애플리케이션에는 `/health`라는 HTTP GET 엔드포인트가 있어서 200(OK)가 반환되면  
  애플리케이션이 정상 작동하고 있다고 알 수 있게 해준다. 이를 사용하기 위해 `Container`에는 `LivenessProbe`가  
  지정되어 있으며 httpGet을 사용하여 path에는 `/health`가 지정되어 있다. 옵션으로는 initialDelaySeconds는 5,  
  periodSeconds는 10, 그리고 failureThreshold는 3을 사용한다.

- K8S가 수행하는 동작은 `ReadinessProbe`에서 본 동작과 HTTP GET 요청을 보낸다는 점만 빼고는 모두 동일하다.  
  `/health`에 애플리케이션 구동 5초 후에 HTTP GET 요청을 10초 간격으로 보냈을 때 500등의 상태 코드와 에러가  
  3번 발생하면 해당 `Pod`를 restart한다.

- yml 파일도 보자.

```yml
# yml file for Pod
apiVersion: v1
kind: Pod
metadata:
  #..
spec:
  containers:
    - name: app2
      image: image/app
      ports:
        - containerPort: 8080
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 5
        periodSeconds: 10
        failureThreshold: 3
```

<hr/>

## Qos Classes

- `Node`안의 3개의 `Pod`(Pod1, Pod2, Pod3)가 `Node`의 자원을 균등하게 분배받아 사용중이라고 가정해보자.  
  이때 Pod1이 추가적인 자원이 필요하게 되었다고 하자. 하지만 `Node`에는 추가적으로 할당받을 수 있는 남은 자원이  
  없기 때문에 Pod1이 리소스 부족으로 에러가 나고, down될 수 있다. 이때, Pod2나 Pod3중 하나를 down 시키고  
  Pod1이 추가적으로 요청한 자원을 할당하도록 할 수도 있다.

- K8S는 애플리케이션의 중요도에 따라 이런 상황을 관리할 수 있도록 세 가지 단계로 Qos(Quality of Service)를 지원한다.

![picture 4](../../images/QOS_SITUATION.png)

- 이 상황에서는 BestEffort가 부여된 Pod3가 가장 먼저 down되어 자원이 반환되고, Pod1은 필요한 추가적인  
  자원을 할당받아 사용할 수 있게 된다.

- BestEffort인 `Pod`가 없는 상황에서 `Pod`가 `Node`에 남아있는 자원량보다 많은 추가적인 자원을 요구하게 된다면  
  Burstable이 적용된 `Pod`가 down되면서 자원이 회수된다.

- 즉, Guaranteed가 마지막까지 `Pod`를 안정적으로 유지시켜주는 Class이다.

- QoS는 특정 속성이 있어서 설정하는 것은 아니고, `Container`에 리소스 관련 설정을 할 때 request, limits에  
  memory, cpu를 어떻게 설정하느냐에 따라 알아서 결정된다.  
  설정하는 방법을 알아보자.

### Guaranteed

- Guaranteed Class로 설정하기 위해서는 `Pod`내의 모든 `Container`에 대해 resources.requests와  
  resources.limits가 명시되어 있어야 한다. 이때, 각각 모두 memory와 cpu가 설정되어 있어야 한다.  
  또한 각 `Container`에 설정된 memory와 cpu값은 resources.requests와 resources.limits에  
  동일한 값으로 지정되어 있어야 한다.

- 위 규칙을 모두 충족해야 K8S가 이 `Pod`를 Guaranteed Class로 판단한다.

### Burstable

- Guaranteed와 BestEffort의 중간 클래스로, 가령 `Container`마다 resources.requests와  
  resources.limits가 설정되어 있지만, requests의 수치가 더 작거나 requests만 설정되어 있거나  
  모든 `Container`마다 requests, limits가 설정되어 있지 않은 경우 등에 해당한다.

### BestEffort

- `Pod`내의 어떠한 `Container`에도 resources.requests, resources.limits가 모두 설정되어 있지  
  않다면, K8S가 이 `Pod`를 BestEffort Class로 판단한다.

![picture 5](../../images/QOS_DEFINING.png)

- Burstable로 판단된 여러 `Pod`들 중 어떤 `Pod`가 먼저 삭제되는지도  
  알아야 하는데, 이는 OOM Score(Out-Of-Memory Score)에 따라 결정된다.  
  위 사진에서 Burstable내에 있는 `Pod`들 중 Pod2와 Pod3를 보자.  
  이 둘은 각각 내부 `Container`의 memory request가 각각 5G, 8G로 설정되어 있는데  
  그 안에 수행되고 있는 애플리케이션이 실제로 사용하고 있는 메모리가 둘 다 4G라고 한다면  
  Pod2의 메모리 사용량은 80%, Pod3의 메모리 사용량은 50%이다. 이 상황에서 OOM Score는  
  Pod2가 80%로 더 크기 때문에 K8S는 **OOM Score가 더 큰 Pod2를 먼저 제거** 한다.

<hr/>

## Node Scheduling

<hr/>
