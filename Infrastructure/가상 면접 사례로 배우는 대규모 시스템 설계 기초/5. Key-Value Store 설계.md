# Key-Value Store 설계

- Key-value store는 key-value 데이터베이스라고도 불리는 비 관계형(non-relational) 데이터베이스이다.  
  이 저장소에 저장되는 값은 고유 식별자(identifier)를 key로 가져야 한다. key와 value사이의 이런 연결 관계를  
  _"key-value"_ Pair(쌍) 이라고 지칭한다.

- Key-value pair에서의 key는 **유일**해야 하며 해당 key에 포함된 value는 key를 통해서만 접근할 수 있다.  
  key는 일반 텍스트일 수도 있고, hash 값일 수도 있다. 성능 상의 이유로, key는 짧을 수록 좋다.  
  아래는 key의 몇 가지 사례이다.

  - 일반 텍스트 key: `last_logged_in_at`
  - Hash key: `253DDEC4`

- key-value pair에서의 value는 문자열일 수도 있고 list일 수도 있고 object일 수도 있다.  
  key-value store는 보통 value로 무엇이 오든 상관하지 않는다. key-value store로 널리 알려진 것으로는  
  Amazon DynamoDB, Memcached, Redis 등이 있다.

- 아래는 key-value store에 저장된 데이터의 예시이다.

| key | value |
| --- | ----- |
| 145 | John  |
| 147 | Bob   |
| 160 | Julia |

- 간단하게 아래의 연산을 지원하는 key-value store를 설계해보자.

  - put(key, value): key-value pair를 저장소에 저장한다.
  - get(key): 인자로 주어진 key에 해당하는 value를 꺼낸다.

---

## 문제 이해 및 설계 범위 확정

- 완벽한 설계란 없다. read, write, 그리고 메모리 사용량 사이에 어떤 균형을 갖고, 데이터의 일관성과 가용성 사이에서  
  타협젹 결정을 내린 설계를 만들었다면 쓸만한 답안일 것이다. 아래의 특성을 갖는 key-value store를 설계해보자.

  - key-value pair의 크기는 10KB 이하이다.
  - 큰 데이터를 저장할 수 있어야 한다.
  - 높은 가용성을 제공해야 한다. 따라서 시스템은 장애가 있더라도 빠르게 응답해야 한다.
  - 높은 규모 확장성을 제공해야 한다. 따라서 트래픽 양에 따라 자동적으로 서버의 증설, 삭제가 이뤄져야 한다.
  - 데이터 일관성 수준은 조정이 가능해야 한다.
  - latency가 짧아야 한다.

---

## 단일 서버 key-value store

- 한 대의 서버만 사용하는 key-value store를 설계하는 것은 쉽다. 가장 직관적인 방법은 key-value pair 전부를  
  메모리에 hash table로 저장하는 것이다. 그러나 이 접근법은 빠른 속도를 보장하긴 하지만 모든 데이터를 메모리 안에  
  두는 것이 불가능할 수도 있다는 약점을 갖고 있다. 이 문제를 해결하기 위해서는 아래의 해결책들이 있다.

  - 데이터 압축(compression)
  - 자주 쓰이는 데이터만 메모리에 두고, 나머지는 disk에 저장

- 그러나 이렇게 개선해도, 한 대의 서버만으로는 부족한 상황이 곧 차자온다.  
  많은 데이터를 저장하려면 Distributed key-value store를 만들 필요가 있다.

---

## Distributed key-value store

- Distributed key-value store는 Distributed hash table이라고도 불린다. key-value pair를 여러 서버에  
  분산시키기 때문이다. 분산 시스템을 설계할 때는 CAP Theorem(Consistency, Availability, Partition Tolerance)를  
  이해하고 있어야 한다.

### CAP Theorem

- CAP Theorem은 Data Consistency(데이터 일관성), Availability(가용성), 그리고 Partition Tolerance(파티션 감래) 라는 세 가지  
  요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리다. 각 요구사항의 의미를 정확하게 알아보자.

  - Data Consistency: 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 볼 수 있어야 한다.
  - Availability: 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
  - Partition Tolerance: 파티션은 두 노드 사이에 통신 장애가 발생했음을 의미한다. 파티션 감내는 네트워크에 파티션이 생기더라도  
    시스템은 계속 동작해야 함을 의미한다.

- CAP Theorem은 아래 표 처럼 이들 가운데 어떤 두 가지를 충족하려면 나머지 하나는 반드시 희생되어야 함을 의미한다.

![picture 1](/images/CAP_THEOREM_1.png)

- key-value store는 위의 3가지 요구사항 중 어느 두 가지를 만족하느냐에 따라 아래처럼 분류할 수 있다.

  - CP System: 일관성과 파티션 감내를 지원하는 key-value store, 가용성을 희생한다.
  - AP System: 가용성과 파티션 감내를 지원하는 key-value store, 일관성을 희생한다.
  - CA System: 일관성과 가용성을 지원하는 key-value store. 파티션 감내는 지원하지 않는다.  
    그러나 통상 네트워크 장애는 피할 수 없는 일로 여겨지므로, 분산 시스템은 반드시 파티션 문제를 감내할 수 있도록  
    설계되어야 한다. 따라서 실세계에 CA System은 존재하지 않는다.

- 위의 정의만으로는 이해하기 어려우니, 몇 가지 구체적인 사례를 살펴보자.  
  분산 시스템에서 데이터는 보통 여러 개의 node에 복제되어 보관된다. 아래 그림처럼 3개의 replica node n1, n2, n3에  
  데이터를 복제해 보관하는 상황을 가정해보자.

#### 이상적인 상태

- *이상적인 환경*이라면 네트워크가 파티션되는 상황은 절대로 일어나지 않을 것이다.  
  n1에 기록된 데이터는 자동적으로 n2와 n3에 복제된다. 데이터 일관성과 가용성도 만족한다.

![picture 2](/images/SDI_KVS_1.png)

#### 실세계의 분산 시스템

- 분산 시스템은 파티션 문제를 피할 수 없다. 그리고 파티션 문제가 발생하면, 우리는 일관성과 가용성 사이에서 하나를 택해야 한다.  
  아래 그림은 n3에 장애가 발생해 n1, n2와 통신할 수 없는 상황을 보여준다. 클라이언트가 n1 또는 n2에 기록한 데이터는 n3에  
  전달되지 않는다. n3에 기록되었으나 아직 n1, n2로 전달되지 않은 데이터가 있다면 n1, n2는 오래된 사본을 갖고 있을 것이다.

![picture 3](/images/SDI_KVS_2.png)

- 가용성 대신 일관성을 선택한다면(CP System) 세 개의 서버 사이에 생길 수 있는 데이터 불일치 문제를 피하기 위해 n1와 n2에  
  대해 쓰기 연산을 중단시켜야 하는데, 그렇게 하면 가용성이 깨진다. 은행권 시스템은 보통 데이터 일관성을 양보하지 않는다.  
  네트워크 파티션 문제 때문에 일관성이 깨질 수 있는 상황이 발생하면 이런 시스템은 상황이 해결될 때까지는 오류를 반환해야 한다.

- 하지만 일관성 대신 가용성을 선택한 AP System은 설사 낡은 데이터를 반환할 위험이 있더라도 계속 read 연산을 허용해야 한다.  
  아울러 n1, n2는 계속 write 연산을 허용할 것이고, 파티션 문제가 해결된 뒤에 새로운 데이터를 n3에 전송할 것이다.

- Distributed key-value store를 만들 때는 그 요구사항에 맞도록 CAP Theorem을 적용해야 한다.

### 시스템 컴포넌트

- Key-Value Store의 구현에 사용될 핵심 컴포넌트들 및 기술들을 살펴보자.

  - Data partition
  - Data replication(다중화)
  - Consistency(일관성)
  - Inconsistency Resolution(일관성 불일치 해소)
  - 장애 처리
  - 시스템 아키텍쳐 다이어그램
  - Write Path(쓰기 경로)
  - Read Path(읽기 경로)

> DynamoDB, Cassandra, BigTable의 사례를 참고한다.

#### Data Partition(데이터 파티션)

- 대규모 애플리케이션의 경우, 전체 데이터를 한 대의 서버에 넣는 것은 불가능하다. 가장 단순한 해결책은 데이터를 작은 partition들로  
  분할한 다음, 여러 대의 서버에 저장하는 것이다. 데이터를 partition 단위로 나눌 때는 아래의 두 가지 문제를 중요하게 따져야 한다.

  - 데이터를 여러 서버에 고르게 분산할 수 있는가
  - node가 추가되거나 삭제될 때 데이터의 이동을 최소화 할 수 있는가

- 위 문제를 푸는 데 적합한 기술이 바로 Consistent Hash이다. Consistent Hash의 동작 원리를 간략하게 다시 보자.

  - 우선 서버를 hash ring에 배치한다. 아래의 hash ring에는 s0, s1, ... s7의 8개 서버가 배치되어 있다.
  - 어떤 key-value pair를 어떤 서버에 저장할지 결정하기 위해 우선 해당 key를 Hash ring위에 배치한다.  
    그 지점으로부터 ring을 시계 방향으로 순회하다 만나는 첫 번째 서버가 바로 해당 key-value pair를 저장할 서버이다.  
    따라서 아래 그림에서 key0은 s1에 저장된다.

![picture 4](/images/SDI_KVS_3.png)

- Consisten Hash를 이용해 데이터를 파티션하면 아래의 장점들이 있다.

  - Automatic scaling(규모 확장 자동화): 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있다.
  - Heterogenicity(다양성): 각 서버의 용량에 맞게 virtual node의 수를 조정할 수 있다.  
    다시 말해, 고성능 서버는 더 많은 virtual node들을 갖도록 설정할 수 있다.

#### Data Replication(데이터 다중화)

- 높은 가용성과 안정성을 확보하기 위해서는 데이터를 N개의 서버에 비동기적으로 다중화(replication)할 필요가 있다.  
  여기서 N은 튜닝 가능한 값이다. N개 서버를 선정하는 방법은 아래와 같다.

  - 어떤 key를 hash ring 위에 배치한 후, 그 지점으로부터 시계 방향으로 ring을 순회하면서 만나는 첫 N개  
    서버에 데이터 사본을 보관한다. 따라서 `N=3`으로 설정한 아래 그림에서 key0은 s1, s2, s3에 저장된다.

![picture 5](/images/SDI_KVS_4.png)

- 그런데 virtual node를 사용한다면 위와 같이 선택한 N개의 node가 대응될 실제 물리 서버의 개수가 N보다  
  작아질 수 있다. 이 문제를 피하려면 node를 선택할 때 같은 물리 서버를 중복 선택하지 않도록 해야한다.

- 같은 데이터 센터에 속한 node는 정전, 네트워크 이슈, 자연재해 등의 문제를 동시에 겪을 가능성이 있다.  
  따라서 안정성을 담보하기 위해 데이터의 사본은 다른 센터의 서버에 보관하고, 센터들은 고속 네트워크로 연결해야 한다.

#### 데이터 일관성

#### 장애 처리

#### 시스템 아키텍쳐 다이어그램

#### Write Path(쓰기 경로)

#### Read Path(읽기 경로)
